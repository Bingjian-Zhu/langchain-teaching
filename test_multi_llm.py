"""
æµ‹è¯•å¤šLLMæ¨¡å‹æ”¯æŒåŠŸèƒ½
éªŒè¯Qwen3å’ŒGeminiæ¨¡å‹çš„é…ç½®å’Œåˆ‡æ¢åŠŸèƒ½
"""
import os
from llm_config import LLMConfig, LLMProvider, list_available_models

def test_llm_config():
    """æµ‹è¯•LLMé…ç½®åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•å¤šLLMæ¨¡å‹æ”¯æŒåŠŸèƒ½")
    print("="*50)
    
    # 1. æµ‹è¯•æ¨¡å‹åˆ—è¡¨
    print("1. æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨:")
    models = LLMConfig.get_available_models()
    for provider_id, display_name in models.items():
        print(f"   - {provider_id}: {display_name}")
    
    # 2. æµ‹è¯•APIå¯†é’¥éªŒè¯
    print("\n2. APIå¯†é’¥çŠ¶æ€æ£€æŸ¥:")
    api_status = LLMConfig.validate_api_keys()
    for provider_id, has_key in api_status.items():
        status = "âœ… å·²è®¾ç½®" if has_key else "âŒ æœªè®¾ç½®"
        display_name = LLMConfig.MODELS[LLMProvider(provider_id)]['display_name']
        print(f"   - {display_name}: {status}")
    
    # 3. æµ‹è¯•å¯ç”¨æ¨¡å‹
    print("\n3. å½“å‰å¯ç”¨çš„æ¨¡å‹:")
    available_models = [
        provider_id for provider_id, has_key in api_status.items() 
        if has_key
    ]
    
    if available_models:
        for model in available_models:
            display_name = LLMConfig.MODELS[LLMProvider(model)]['display_name']
            print(f"   âœ… {display_name} ({model})")
    else:
        print("   âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ï¼Œéœ€è¦è®¾ç½®APIå¯†é’¥")
    
    # 4. æµ‹è¯•æ¨¡å‹åˆ›å»ºï¼ˆä»…å¯¹æœ‰APIå¯†é’¥çš„æ¨¡å‹ï¼‰
    print("\n4. æ¨¡å‹åˆ›å»ºæµ‹è¯•:")
    for model in available_models:
        try:
            from llm_config import get_llm_by_name
            llm = get_llm_by_name(model)
            display_name = LLMConfig.MODELS[LLMProvider(model)]['display_name']
            print(f"   âœ… {display_name} æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"   âŒ {model} æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
    
    return available_models

def demo_llm_usage(available_models):
    """æ¼”ç¤ºLLMä½¿ç”¨"""
    if not available_models:
        print("\nâŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹è¿›è¡Œæ¼”ç¤º")
        return
    
    print(f"\nğŸ¯ LLMåŠŸèƒ½æ¼”ç¤º (ä½¿ç”¨ {available_models[0]})")
    print("="*50)
    
    try:
        from teaching_system import IntelligentTutoringSystem
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹åˆ›å»ºç³»ç»Ÿ
        system = IntelligentTutoringSystem(llm_provider=available_models[0])
        
        # æµ‹è¯•ç®€å•çš„é—®ç­”
        test_question = "ä»€ä¹ˆæ˜¯2+3ï¼Ÿ"
        test_answer = "5"
        knowledge_points = ["åŸºç¡€åŠ æ³•"]
        
        print(f"æµ‹è¯•é—®é¢˜: {test_question}")
        print(f"å­¦ç”Ÿç­”æ¡ˆ: {test_answer}")
        print("AIè¯„åˆ†ä¸­...")
        
        result = system.grade_answer(test_question, "5", test_answer, knowledge_points)
        
        print(f"è¯„åˆ†ç»“æœ: {result['score']}/10")
        print(f"åˆ†æ: {result['analysis']}")
        
        print("\nâœ… LLMåŠŸèƒ½æµ‹è¯•æˆåŠŸï¼")
        
    except Exception as e:
        print(f"âŒ LLMåŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    available_models = test_llm_config()
    demo_llm_usage(available_models)
    
    print("\n" + "="*50)
    print("ğŸ‰ å¤šLLMæ”¯æŒåŠŸèƒ½æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print("1. è¿è¡Œ python main_system.py å¯åŠ¨å®Œæ•´ç³»ç»Ÿ")
    print("2. ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„LLMæ¨¡å‹")
    print("3. å¦‚æœæœ‰å¤šä¸ªæ¨¡å‹å¯ç”¨ï¼Œç³»ç»Ÿä¼šè®©ä½ é€‰æ‹©")
    print("4. æ”¯æŒåœ¨Qwen3å’ŒGeminiä¹‹é—´åˆ‡æ¢")
